{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion de Sistema de Recomendacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar un sistema de recomendacion es una tarea compleja. En definitiva queremos saber si lo que recomendamos es realmente util para el usuario, lo cual es algo netamente subjetivo. Por esta razon es dificil construir metricas que sirvan para evaluar y comparar un sistema de recomendaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una paradoja interesante es la siguiente: Evaluamos un sistema de recomendacion en base a las calificaciones que los usuarios hicieron de ciertos items, esto supone que los usuario son \"perfectos\". Pero si los usuarios son perfectos entonces ya consumieron todos los items que les resultaban interesantes y no necesitamos recomendarles cosas nuevas. Visto de otra forma, evaluamos los sistemas de recomendacion en base a lo que los usuarios hicieron en el pasado suponiendo que un buen sistema es aquel que es capaz de predecir lo que el usuario consumio. Esto tiene una falla fatal dado que a lo mejor los usuarios hubieran consumido otras cosas si se las hubieramos recomendado. Como podemos ver el tema no es nada sencillo y hay que tomar con extremo cuidado el resultado de cualquier metrica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta seccion veremos algunas de las metricas populares para intentar hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluacion basada en las predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primer familia de metricas se basa en realizar predicciones para los items que los usuarios ya han calificado usando el sistema y luego comparar estas predicciones con los ratings conocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La soluccion a este problema es bastante simple y consiste en separar un cierto porcentaje de calificaciones al azar, por ejemplo 20% del total. Con el 80% restante construimos nuestro sistema de recomendaciones y luego lo probamos realizando predicciones para las calificaciones que hemos removido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/particion_test_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Como conocemos el valor real de estas calificaciones podemos calcular la efectividad del sistema mediante un simple calculo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/formula_calcular_efectividad.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A esta metrica se la denomina <b>RMSE</b> (root mean squared error). Tambien puede calcularse sin la raiz en cuyo caso se denomina <b>MSE</b>. El cuadrado cumple la funcion de hacer que el error sea mayor a medida que la diferencia entre la prediccion y el valor conocido para el raiting aumenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gracias</b> a esta metrica podemos probar diferentes algoritmos y parametros para nuestros algoritmos de forma tal de encontrar el algoritmo que minimice el error total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es recomendable realizar <b>varios</b> test tomando cada vez un 80% y 20% al azar de las calificaciones y calcular el promedio de error de todas las pruebas. De esta forma evitamos que un cierto muestreo de las calificaciones sea muy influyente en cuanto al rendimiento del sistema. A esta tecnica en aprendisaje automatico se la llama <b>cross validation</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La principal critica a las metricas basadas en <b>RMSE</b> o <b>MSE</b> es que ciertos errores no son tan importantes como otros, por ejemplo si un usuario le dio 5 estrellas a una pelicula y el sistema predice 4 tenemos un error igual a 1, que es el mismo entre darle 2.5 y 3.5 a una pelicula. Sin embargo el primero es bastante mas importante que el segundo ya que a la hora de recomendar queremos encontrar las peliculas de 5 estrellas. En definitiva cometer errores con las peliculas que mas le gustaron al usuario es mas grave que cometer errores sobre las peliculas que lo le han gustado o con calificaciones intermedias. Las metricas basadas en el orden intentan atacar este problema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
